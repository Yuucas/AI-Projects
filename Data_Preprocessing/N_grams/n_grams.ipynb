{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Category Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Get the stop words list\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "# Get the punctuation list\n",
    "punctuation_list = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset \n",
    "\n",
    "Dataset Link: https://www.kaggle.com/code/hengzheng/news-category-classifier-val-acc-0-65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                            headline       date  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week... 2018-05-26   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2... 2018-05-26   \n",
       "2    Hugh Grant Marries For The First Time At Age 57 2018-05-26   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D... 2018-05-26   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags... 2018-05-26   \n",
       "\n",
       "                                                link          authors  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...  Melissa Jeltsen   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...    Andy McDonald   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...       Ron Dicker   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...       Ron Dicker   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...       Ron Dicker   \n",
       "\n",
       "        category  \n",
       "0          CRIME  \n",
       "1  ENTERTAINMENT  \n",
       "2  ENTERTAINMENT  \n",
       "3  ENTERTAINMENT  \n",
       "4  ENTERTAINMENT  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the json file\n",
    "df = pd.read_json(r\"news_category.json\", lines=True)\n",
    "\n",
    "df_original = df\n",
    "\n",
    "# Illustrate the top 5 column\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "POLITICS          32739\n",
       "ENTERTAINMENT     14257\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       4995\n",
       "BUSINESS           4254\n",
       "SPORTS             4167\n",
       "COMEDY             3971\n",
       "PARENTS            3955\n",
       "BLACK VOICES       3858\n",
       "THE WORLDPOST      3664\n",
       "WOMEN              3490\n",
       "CRIME              2893\n",
       "MEDIA              2815\n",
       "WEIRD NEWS         2670\n",
       "GREEN              2622\n",
       "IMPACT             2602\n",
       "WORLDPOST          2579\n",
       "RELIGION           2556\n",
       "STYLE              2254\n",
       "WORLD NEWS         2177\n",
       "TRAVEL             2145\n",
       "TASTE              2096\n",
       "ARTS               1509\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "SCIENCE            1381\n",
       "ARTS & CULTURE     1339\n",
       "TECH               1231\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1129\n",
       "EDUCATION          1004\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the each category\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Labels:  {'CRIME': 0, 'ENTERTAINMENT': 1, 'WORLD NEWS': 2, 'IMPACT': 3, 'POLITICS': 4, 'WEIRD NEWS': 5, 'BLACK VOICES': 6, 'WOMEN': 7, 'COMEDY': 8, 'QUEER VOICES': 9, 'SPORTS': 10, 'BUSINESS': 11, 'TRAVEL': 12, 'MEDIA': 13, 'TECH': 14, 'RELIGION': 15, 'SCIENCE': 16, 'LATINO VOICES': 17, 'EDUCATION': 18, 'COLLEGE': 19, 'PARENTS': 20, 'ARTS & CULTURE': 21, 'STYLE': 22, 'GREEN': 23, 'TASTE': 24, 'HEALTHY LIVING': 25, 'THE WORLDPOST': 26, 'GOOD NEWS': 27, 'WORLDPOST': 28, 'FIFTY': 29, 'ARTS': 30}\n"
     ]
    }
   ],
   "source": [
    "# Get unique label names from category\n",
    "unique_labels = df.category.unique()\n",
    "# Define the numerical labels for each category\n",
    "numerical_labels = [x for x in range(len(unique_labels))]\n",
    "# Convert text category into numerical category\n",
    "numerical_target_dict = dict(zip(unique_labels, numerical_labels))\n",
    "print(\"Numerical Labels: \", numerical_target_dict)\n",
    "\n",
    "# Add the new numerical label into the dataframe\n",
    "df['category_label'] = df['category'].map(numerical_target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>category_label</th>\n",
       "      <th>headline_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>2 Mass Shootings Texas Week, 1 TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>Smith Joins Diplo Nicky Jam 2018 World Cup's O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>Hugh Grant Marries Time Age 57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff Democ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                            headline       date  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week... 2018-05-26   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2... 2018-05-26   \n",
       "2    Hugh Grant Marries For The First Time At Age 57 2018-05-26   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D... 2018-05-26   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags... 2018-05-26   \n",
       "\n",
       "                                                link          authors  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...  Melissa Jeltsen   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...    Andy McDonald   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...       Ron Dicker   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...       Ron Dicker   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...       Ron Dicker   \n",
       "\n",
       "        category  category_label  \\\n",
       "0          CRIME               0   \n",
       "1  ENTERTAINMENT               1   \n",
       "2  ENTERTAINMENT               1   \n",
       "3  ENTERTAINMENT               1   \n",
       "4  ENTERTAINMENT               1   \n",
       "\n",
       "                                   headline_adjusted  \n",
       "0                  2 Mass Shootings Texas Week, 1 TV  \n",
       "1  Smith Joins Diplo Nicky Jam 2018 World Cup's O...  \n",
       "2                     Hugh Grant Marries Time Age 57  \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff Democ...  \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the redundant words from dataset\n",
    "df['headline_adjusted'] = df['headline'].apply(lambda x: x.split())\n",
    "# Remove the stop words\n",
    "df['headline_adjusted'] = df['headline_adjusted'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "# Remove the punctuations \n",
    "df['headline_adjusted'] = df['headline_adjusted'].apply(lambda x: [word for word in x if word.lower() not in punctuation_list])\n",
    "df['headline_adjusted'] = df['headline_adjusted'].apply(' '.join)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training wo/Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54       579\n",
      "           1       0.48      0.84      0.61      2851\n",
      "           2       0.49      0.09      0.16       435\n",
      "           3       0.38      0.10      0.15       520\n",
      "           4       0.56      0.93      0.69      6548\n",
      "           5       0.42      0.21      0.28       534\n",
      "           6       0.52      0.26      0.34       772\n",
      "           7       0.51      0.20      0.28       698\n",
      "           8       0.65      0.26      0.37       794\n",
      "           9       0.66      0.56      0.61       999\n",
      "          10       0.72      0.58      0.64       833\n",
      "          11       0.55      0.35      0.43       851\n",
      "          12       0.76      0.32      0.45       429\n",
      "          13       0.72      0.25      0.37       563\n",
      "          14       0.71      0.11      0.19       246\n",
      "          15       0.74      0.28      0.41       511\n",
      "          16       0.83      0.22      0.35       276\n",
      "          17       0.80      0.04      0.07       226\n",
      "          18       0.79      0.07      0.14       201\n",
      "          19       0.69      0.10      0.18       229\n",
      "          20       0.52      0.45      0.48       791\n",
      "          21       0.57      0.06      0.11       268\n",
      "          22       0.68      0.27      0.39       451\n",
      "          23       0.53      0.23      0.32       524\n",
      "          24       0.77      0.48      0.60       419\n",
      "          25       0.46      0.61      0.53      1339\n",
      "          26       0.45      0.50      0.47       733\n",
      "          27       0.63      0.13      0.21       280\n",
      "          28       0.50      0.12      0.20       516\n",
      "          29       0.69      0.03      0.06       280\n",
      "          30       0.66      0.12      0.21       302\n",
      "\n",
      "    accuracy                           0.54     24998\n",
      "   macro avg       0.61      0.30      0.35     24998\n",
      "weighted avg       0.57      0.54      0.49     24998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"headline_adjusted\"], df['category_label'], test_size=0.2, random_state=331, stratify=df.category_label)\n",
    "\n",
    "# Create pipeline\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.30      0.42       579\n",
      "           1       0.45      0.84      0.59      2851\n",
      "           2       0.57      0.04      0.07       435\n",
      "           3       0.57      0.02      0.04       520\n",
      "           4       0.41      0.98      0.58      6548\n",
      "           5       0.58      0.07      0.12       534\n",
      "           6       0.69      0.11      0.19       772\n",
      "           7       0.75      0.08      0.14       698\n",
      "           8       0.83      0.11      0.20       794\n",
      "           9       0.84      0.36      0.51       999\n",
      "          10       0.81      0.38      0.52       833\n",
      "          11       0.78      0.15      0.25       851\n",
      "          12       0.83      0.15      0.26       429\n",
      "          13       0.80      0.09      0.15       563\n",
      "          14       0.60      0.02      0.05       246\n",
      "          15       0.81      0.12      0.20       511\n",
      "          16       0.88      0.05      0.10       276\n",
      "          17       1.00      0.00      0.01       226\n",
      "          18       0.83      0.02      0.05       201\n",
      "          19       0.60      0.01      0.03       229\n",
      "          20       0.70      0.25      0.37       791\n",
      "          21       0.25      0.00      0.01       268\n",
      "          22       0.77      0.12      0.20       451\n",
      "          23       0.70      0.07      0.13       524\n",
      "          24       0.84      0.32      0.47       419\n",
      "          25       0.58      0.40      0.47      1339\n",
      "          26       0.58      0.27      0.36       733\n",
      "          27       0.71      0.04      0.08       280\n",
      "          28       0.68      0.03      0.05       516\n",
      "          29       1.00      0.01      0.01       280\n",
      "          30       0.93      0.05      0.09       302\n",
      "\n",
      "    accuracy                           0.46     24998\n",
      "   macro avg       0.71      0.18      0.22     24998\n",
      "weighted avg       0.61      0.46      0.37     24998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"headline_adjusted\"], df['category_label'], test_size=0.2, random_state=331, stratify=df.category_label)\n",
    "\n",
    "# Create pipeline\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.26      0.37       579\n",
      "           1       0.46      0.83      0.59      2851\n",
      "           2       0.56      0.03      0.06       435\n",
      "           3       0.63      0.02      0.04       520\n",
      "           4       0.39      0.99      0.56      6548\n",
      "           5       0.67      0.05      0.10       534\n",
      "           6       0.68      0.09      0.16       772\n",
      "           7       0.79      0.07      0.12       698\n",
      "           8       0.82      0.10      0.18       794\n",
      "           9       0.86      0.33      0.48       999\n",
      "          10       0.83      0.33      0.48       833\n",
      "          11       0.80      0.12      0.21       851\n",
      "          12       0.84      0.14      0.25       429\n",
      "          13       0.87      0.08      0.15       563\n",
      "          14       0.62      0.02      0.04       246\n",
      "          15       0.84      0.10      0.18       511\n",
      "          16       0.78      0.05      0.10       276\n",
      "          17       0.00      0.00      0.00       226\n",
      "          18       0.71      0.02      0.05       201\n",
      "          19       0.33      0.00      0.01       229\n",
      "          20       0.71      0.22      0.33       791\n",
      "          21       0.33      0.00      0.01       268\n",
      "          22       0.80      0.12      0.21       451\n",
      "          23       0.74      0.05      0.10       524\n",
      "          24       0.84      0.29      0.43       419\n",
      "          25       0.61      0.36      0.45      1339\n",
      "          26       0.62      0.24      0.35       733\n",
      "          27       0.69      0.04      0.07       280\n",
      "          28       0.75      0.02      0.03       516\n",
      "          29       1.00      0.01      0.01       280\n",
      "          30       0.94      0.05      0.09       302\n",
      "\n",
      "    accuracy                           0.45     24998\n",
      "   macro avg       0.68      0.16      0.20     24998\n",
      "weighted avg       0.61      0.45      0.36     24998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yukse\\Desktop\\Yuksel\\Yucas\\NLP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yukse\\Desktop\\Yuksel\\Yucas\\NLP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\yukse\\Desktop\\Yuksel\\Yucas\\NLP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"headline_adjusted\"], df['category_label'], test_size=0.2, random_state=331, stratify=df.category_label)\n",
    "\n",
    "# Create pipeline\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training w/Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.68      0.52       579\n",
      "           1       0.76      0.57      0.65      2851\n",
      "           2       0.25      0.35      0.29       435\n",
      "           3       0.23      0.31      0.26       520\n",
      "           4       0.89      0.57      0.69      6548\n",
      "           5       0.31      0.33      0.32       534\n",
      "           6       0.40      0.45      0.42       772\n",
      "           7       0.35      0.35      0.35       698\n",
      "           8       0.38      0.45      0.41       794\n",
      "           9       0.63      0.62      0.62       999\n",
      "          10       0.67      0.69      0.68       833\n",
      "          11       0.42      0.44      0.43       851\n",
      "          12       0.49      0.57      0.53       429\n",
      "          13       0.39      0.59      0.47       563\n",
      "          14       0.28      0.46      0.35       246\n",
      "          15       0.46      0.55      0.50       511\n",
      "          16       0.35      0.43      0.38       276\n",
      "          17       0.27      0.44      0.33       226\n",
      "          18       0.21      0.41      0.28       201\n",
      "          19       0.30      0.45      0.36       229\n",
      "          20       0.49      0.52      0.51       791\n",
      "          21       0.23      0.37      0.29       268\n",
      "          22       0.44      0.61      0.51       451\n",
      "          23       0.39      0.44      0.41       524\n",
      "          24       0.57      0.73      0.64       419\n",
      "          25       0.58      0.47      0.52      1339\n",
      "          26       0.43      0.51      0.47       733\n",
      "          27       0.25      0.44      0.32       280\n",
      "          28       0.33      0.37      0.35       516\n",
      "          29       0.20      0.33      0.25       280\n",
      "          30       0.32      0.32      0.32       302\n",
      "\n",
      "    accuracy                           0.52     24998\n",
      "   macro avg       0.41      0.48      0.43     24998\n",
      "weighted avg       0.58      0.52      0.54     24998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"headline_adjusted\"], df['category_label'], test_size=0.2, random_state=331, stratify=df.category_label)\n",
    "\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('rndsmplr', RandomOverSampler()),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.70      0.52       579\n",
      "           1       0.78      0.60      0.68      2851\n",
      "           2       0.27      0.37      0.31       435\n",
      "           3       0.26      0.31      0.29       520\n",
      "           4       0.89      0.59      0.71      6548\n",
      "           5       0.32      0.33      0.32       534\n",
      "           6       0.44      0.51      0.47       772\n",
      "           7       0.37      0.36      0.36       698\n",
      "           8       0.39      0.47      0.43       794\n",
      "           9       0.65      0.69      0.67       999\n",
      "          10       0.68      0.72      0.70       833\n",
      "          11       0.48      0.44      0.46       851\n",
      "          12       0.50      0.58      0.54       429\n",
      "          13       0.40      0.60      0.48       563\n",
      "          14       0.31      0.50      0.39       246\n",
      "          15       0.47      0.52      0.49       511\n",
      "          16       0.36      0.44      0.40       276\n",
      "          17       0.25      0.41      0.31       226\n",
      "          18       0.25      0.45      0.32       201\n",
      "          19       0.30      0.47      0.37       229\n",
      "          20       0.52      0.54      0.53       791\n",
      "          21       0.21      0.37      0.27       268\n",
      "          22       0.43      0.62      0.51       451\n",
      "          23       0.41      0.46      0.43       524\n",
      "          24       0.56      0.74      0.64       419\n",
      "          25       0.61      0.48      0.54      1339\n",
      "          26       0.44      0.55      0.49       733\n",
      "          27       0.26      0.47      0.33       280\n",
      "          28       0.34      0.37      0.35       516\n",
      "          29       0.22      0.30      0.25       280\n",
      "          30       0.35      0.32      0.33       302\n",
      "\n",
      "    accuracy                           0.54     24998\n",
      "   macro avg       0.42      0.49      0.45     24998\n",
      "weighted avg       0.60      0.54      0.55     24998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"headline_adjusted\"], df['category_label'], test_size=0.2, random_state=331, stratify=df.category_label)\n",
    "\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('rndsmplr', RandomOverSampler()),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.71      0.53       579\n",
      "           1       0.79      0.60      0.68      2851\n",
      "           2       0.26      0.38      0.31       435\n",
      "           3       0.26      0.31      0.29       520\n",
      "           4       0.89      0.59      0.71      6548\n",
      "           5       0.33      0.34      0.33       534\n",
      "           6       0.44      0.51      0.47       772\n",
      "           7       0.36      0.36      0.36       698\n",
      "           8       0.39      0.47      0.42       794\n",
      "           9       0.65      0.68      0.66       999\n",
      "          10       0.69      0.73      0.71       833\n",
      "          11       0.48      0.44      0.46       851\n",
      "          12       0.51      0.59      0.54       429\n",
      "          13       0.39      0.62      0.48       563\n",
      "          14       0.30      0.49      0.37       246\n",
      "          15       0.48      0.53      0.50       511\n",
      "          16       0.36      0.46      0.40       276\n",
      "          17       0.27      0.42      0.33       226\n",
      "          18       0.24      0.44      0.32       201\n",
      "          19       0.30      0.47      0.36       229\n",
      "          20       0.52      0.54      0.53       791\n",
      "          21       0.22      0.38      0.28       268\n",
      "          22       0.43      0.61      0.50       451\n",
      "          23       0.41      0.48      0.44       524\n",
      "          24       0.57      0.72      0.64       419\n",
      "          25       0.62      0.48      0.54      1339\n",
      "          26       0.45      0.54      0.49       733\n",
      "          27       0.26      0.47      0.34       280\n",
      "          28       0.34      0.37      0.36       516\n",
      "          29       0.21      0.29      0.24       280\n",
      "          30       0.33      0.31      0.32       302\n",
      "\n",
      "    accuracy                           0.54     24998\n",
      "   macro avg       0.43      0.49      0.45     24998\n",
      "weighted avg       0.60      0.54      0.56     24998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"headline_adjusted\"], df['category_label'], test_size=0.2, random_state=331, stratify=df.category_label)\n",
    "\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('rndsmplr', RandomOverSampler()),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training w/Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>category_label</th>\n",
       "      <th>headline_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92276</th>\n",
       "      <td></td>\n",
       "      <td>Prisoner In Van Said Freddie Gray Was ‘Banging...</td>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/prisoner-...</td>\n",
       "      <td></td>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>Prisoner Van Said Freddie Gray ‘Banging Walls'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56315</th>\n",
       "      <td>“The one thing that we can say is that this is...</td>\n",
       "      <td>New Details Emerge About Deadliest Mass Shooti...</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/terror-sh...</td>\n",
       "      <td>Sebastian Murdock, Andy Campbell, Roque Planas...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>New Details Emerge Deadliest Mass Shooting U.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39620</th>\n",
       "      <td>The child was sitting in the back of his grand...</td>\n",
       "      <td>Road-Rage Shooting Leaves 3-Year-Old Boy Dead,...</td>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/toddler-k...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>Road-Rage Shooting Leaves 3-Year-Old Boy Dead,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100579</th>\n",
       "      <td></td>\n",
       "      <td>1-Month-Old's Face Mauled By Ferrets In Philad...</td>\n",
       "      <td>2015-01-24</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/ferrets-a...</td>\n",
       "      <td></td>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>1-Month-Old's Face Mauled Ferrets Philadelphia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114843</th>\n",
       "      <td>American prisons foster a culture of violence,...</td>\n",
       "      <td>The Dirt Wars: An Intimate Look at Convict Cul...</td>\n",
       "      <td>2014-08-13</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-dirt-...</td>\n",
       "      <td>Christopher Zoukis, ContributorAuthor, Federal...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>Dirt Wars: Intimate Look Convict Culture Ameri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        short_description  \\\n",
       "92276                                                       \n",
       "56315   “The one thing that we can say is that this is...   \n",
       "39620   The child was sitting in the back of his grand...   \n",
       "100579                                                      \n",
       "114843  American prisons foster a culture of violence,...   \n",
       "\n",
       "                                                 headline       date  \\\n",
       "92276   Prisoner In Van Said Freddie Gray Was ‘Banging... 2015-04-30   \n",
       "56315   New Details Emerge About Deadliest Mass Shooti... 2016-06-12   \n",
       "39620   Road-Rage Shooting Leaves 3-Year-Old Boy Dead,... 2016-12-18   \n",
       "100579  1-Month-Old's Face Mauled By Ferrets In Philad... 2015-01-24   \n",
       "114843  The Dirt Wars: An Intimate Look at Convict Cul... 2014-08-13   \n",
       "\n",
       "                                                     link  \\\n",
       "92276   https://www.huffingtonpost.com/entry/prisoner-...   \n",
       "56315   https://www.huffingtonpost.com/entry/terror-sh...   \n",
       "39620   https://www.huffingtonpost.com/entry/toddler-k...   \n",
       "100579  https://www.huffingtonpost.com/entry/ferrets-a...   \n",
       "114843  https://www.huffingtonpost.com/entry/the-dirt-...   \n",
       "\n",
       "                                                  authors category  \\\n",
       "92276                                                        CRIME   \n",
       "56315   Sebastian Murdock, Andy Campbell, Roque Planas...    CRIME   \n",
       "39620                                      Nina Golgowski    CRIME   \n",
       "100579                                                       CRIME   \n",
       "114843  Christopher Zoukis, ContributorAuthor, Federal...    CRIME   \n",
       "\n",
       "        category_label                                  headline_adjusted  \n",
       "92276                0  Prisoner Van Said Freddie Gray ‘Banging Walls'...  \n",
       "56315                0  New Details Emerge Deadliest Mass Shooting U.S...  \n",
       "39620                0  Road-Rage Shooting Leaves 3-Year-Old Boy Dead,...  \n",
       "100579               0  1-Month-Old's Face Mauled Ferrets Philadelphia...  \n",
       "114843               0  Dirt Wars: Intimate Look Convict Culture Ameri...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty dataframe\n",
    "df_resampled = pd.DataFrame()\n",
    "# Set minimum number of sample\n",
    "min_samples = 1004 \n",
    "\n",
    "\n",
    "for label in unique_labels:\n",
    "    df_new = df[df.category==label].sample(min_samples, random_state=331)\n",
    "    df_resampled = pd.concat([df_resampled,df_new],axis=0)\n",
    "\n",
    "# Convert text category into numerical category\n",
    "numerical_target_dict = dict(zip(unique_labels, numerical_labels))\n",
    "\n",
    "# Add the new numerical label into the dataframe\n",
    "df_resampled['category_label'] = df_resampled['category'].map(numerical_target_dict)\n",
    "\n",
    "df_resampled['headline_adjusted'] = df_resampled['headline'].apply(lambda x: x.split())\n",
    "# Remove the stop words\n",
    "df_resampled['headline_adjusted'] = df_resampled['headline_adjusted'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "# Remove the punctuations \n",
    "df_resampled['headline_adjusted'] = df_resampled['headline_adjusted'].apply(lambda x: [word for word in x if word.lower() not in punctuation_list])\n",
    "df_resampled['headline_adjusted'] = df_resampled['headline_adjusted'].apply(' '.join)\n",
    "\n",
    "df_resampled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.74      0.55       201\n",
      "           1       0.40      0.39      0.39       201\n",
      "           2       0.42      0.44      0.43       201\n",
      "           3       0.24      0.19      0.21       201\n",
      "           4       0.38      0.47      0.42       201\n",
      "           5       0.39      0.27      0.32       201\n",
      "           6       0.51      0.43      0.47       201\n",
      "           7       0.37      0.35      0.36       201\n",
      "           8       0.42      0.50      0.46       200\n",
      "           9       0.54      0.58      0.56       200\n",
      "          10       0.61      0.60      0.61       201\n",
      "          11       0.45      0.37      0.40       201\n",
      "          12       0.52      0.52      0.52       201\n",
      "          13       0.52      0.55      0.53       201\n",
      "          14       0.52      0.64      0.57       201\n",
      "          15       0.62      0.47      0.54       200\n",
      "          16       0.52      0.47      0.50       201\n",
      "          17       0.63      0.54      0.58       200\n",
      "          18       0.52      0.53      0.52       201\n",
      "          19       0.56      0.55      0.55       201\n",
      "          20       0.45      0.51      0.47       201\n",
      "          21       0.36      0.40      0.38       201\n",
      "          22       0.50      0.60      0.55       201\n",
      "          23       0.43      0.41      0.42       200\n",
      "          24       0.58      0.62      0.60       201\n",
      "          25       0.38      0.31      0.34       201\n",
      "          26       0.38      0.40      0.39       201\n",
      "          27       0.38      0.52      0.43       200\n",
      "          28       0.42      0.28      0.34       201\n",
      "          29       0.41      0.36      0.38       201\n",
      "          30       0.44      0.26      0.33       201\n",
      "\n",
      "    accuracy                           0.46      6225\n",
      "   macro avg       0.46      0.46      0.46      6225\n",
      "weighted avg       0.46      0.46      0.46      6225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_resampled[\"headline_adjusted\"], df_resampled['category_label'], test_size=0.2, random_state=42, stratify=df_resampled.category_label)\n",
    "\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.73      0.54       201\n",
      "           1       0.41      0.43      0.42       201\n",
      "           2       0.43      0.47      0.45       201\n",
      "           3       0.24      0.18      0.21       201\n",
      "           4       0.37      0.49      0.42       201\n",
      "           5       0.37      0.26      0.31       201\n",
      "           6       0.53      0.50      0.52       201\n",
      "           7       0.37      0.33      0.35       201\n",
      "           8       0.43      0.50      0.46       200\n",
      "           9       0.55      0.59      0.57       200\n",
      "          10       0.62      0.64      0.63       201\n",
      "          11       0.43      0.35      0.39       201\n",
      "          12       0.52      0.49      0.50       201\n",
      "          13       0.51      0.59      0.55       201\n",
      "          14       0.52      0.65      0.58       201\n",
      "          15       0.66      0.48      0.56       200\n",
      "          16       0.56      0.47      0.51       201\n",
      "          17       0.61      0.55      0.58       200\n",
      "          18       0.53      0.51      0.52       201\n",
      "          19       0.57      0.56      0.56       201\n",
      "          20       0.46      0.50      0.48       201\n",
      "          21       0.34      0.38      0.36       201\n",
      "          22       0.50      0.56      0.53       201\n",
      "          23       0.43      0.40      0.41       200\n",
      "          24       0.58      0.61      0.59       201\n",
      "          25       0.40      0.32      0.35       201\n",
      "          26       0.38      0.37      0.37       201\n",
      "          27       0.37      0.52      0.43       200\n",
      "          28       0.44      0.32      0.37       201\n",
      "          29       0.43      0.35      0.39       201\n",
      "          30       0.41      0.23      0.30       201\n",
      "\n",
      "    accuracy                           0.46      6225\n",
      "   macro avg       0.46      0.46      0.46      6225\n",
      "weighted avg       0.46      0.46      0.46      6225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_resampled[\"headline_adjusted\"], df_resampled['category_label'], test_size=0.2, random_state=42, stratify=df_resampled.category_label)\n",
    "\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.72      0.51       201\n",
      "           1       0.41      0.41      0.41       201\n",
      "           2       0.41      0.46      0.43       201\n",
      "           3       0.25      0.18      0.21       201\n",
      "           4       0.37      0.49      0.42       201\n",
      "           5       0.39      0.25      0.30       201\n",
      "           6       0.54      0.49      0.51       201\n",
      "           7       0.37      0.34      0.36       201\n",
      "           8       0.43      0.50      0.46       200\n",
      "           9       0.53      0.60      0.56       200\n",
      "          10       0.62      0.64      0.63       201\n",
      "          11       0.46      0.35      0.40       201\n",
      "          12       0.53      0.50      0.52       201\n",
      "          13       0.51      0.60      0.55       201\n",
      "          14       0.52      0.66      0.58       201\n",
      "          15       0.65      0.48      0.55       200\n",
      "          16       0.54      0.47      0.51       201\n",
      "          17       0.60      0.54      0.56       200\n",
      "          18       0.53      0.51      0.52       201\n",
      "          19       0.56      0.53      0.54       201\n",
      "          20       0.46      0.52      0.49       201\n",
      "          21       0.36      0.39      0.38       201\n",
      "          22       0.49      0.56      0.52       201\n",
      "          23       0.44      0.41      0.43       200\n",
      "          24       0.56      0.60      0.58       201\n",
      "          25       0.42      0.33      0.37       201\n",
      "          26       0.37      0.37      0.37       201\n",
      "          27       0.37      0.51      0.43       200\n",
      "          28       0.45      0.32      0.37       201\n",
      "          29       0.42      0.34      0.37       201\n",
      "          30       0.44      0.24      0.31       201\n",
      "\n",
      "    accuracy                           0.46      6225\n",
      "   macro avg       0.46      0.46      0.46      6225\n",
      "weighted avg       0.46      0.46      0.46      6225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_resampled[\"headline_adjusted\"], df_resampled['category_label'], test_size=0.2, random_state=42, stratify=df_resampled.category_label)\n",
    "\n",
    "textclassifier =Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "('mnb', MultinomialNB(alpha =1.0))\n",
    "])\n",
    "\n",
    "# Start the training\n",
    "textclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
